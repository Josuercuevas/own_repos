# Compressed Domain Diffusion Model

The idea is that instead of using an AutoEncoder topology at the beginning of the diffusion process we could simply leverage the compressed domain of the input (images, videos, audio). This will reduce computation significantly speeding up trainig and inference.


# References

1. [Latent Diffusion Model paper](https://arxiv.org/pdf/2112.10752.pdf)

2. [Latent Diffusion Model Code](https://github.com/CompVis/latent-diffusion)

3. [Transformers paper](https://arxiv.org/pdf/1706.03762.pdf)

4. [Transformers tutorial](https://theaisummer.com/transformer/)

5. [Positional Embeddings](https://theaisummer.com/positional-embeddings/)

6. [Diffusion Nvidia Blog](https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/)

7. [Compressed domain object detection and feature extraction](https://www.researchgate.net/publication/358137310_Usage_of_compressed_domain_in_fast_frameworks)

8. [jpeg2dct](https://github.com/uber-research/jpeg2dct)

9. [Faster and Accurate Compressed Video Action Recognition Straight from the Frequency Domain](https://arxiv.org/pdf/2012.13726.pdf)



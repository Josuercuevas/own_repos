name: 'SqueezeDet_v2'

# User has to
# substract mean value BGR format [103.94,116.78,123.68]
# swap channels opencv [B, H, W, C] to [B, C, H, W]
# follow the example test we provide in the folder
#

input: "data"
input_shape {
  dim: 1
  dim: 3
  dim: 480
  dim: 640
}

# ==============================================
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu_conv1"
}

# ==============================================
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu_conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

# ==============================================
layer {
  name: "fire2_squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2_squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
  }
}
layer {
  name: "fire2_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2_squeeze1x1"
  top: "fire2_squeeze1x1"
}
layer {
  name: "fire2_expand1x1"
  type: "Convolution"
  bottom: "fire2_squeeze1x1"
  top: "fire2_expand1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}
layer {
  name: "fire2_relu_expand1x1"
  type: "ReLU"
  bottom: "fire2_expand1x1"
  top: "fire2_expand1x1"
}
layer {
  name: "fire2_expand3x3"
  type: "Convolution"
  bottom: "fire2_squeeze1x1"
  top: "fire2_expand3x3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire2_relu_expand3x3"
  type: "ReLU"
  bottom: "fire2_expand3x3"
  top: "fire2_expand3x3"
}
layer {
  name: "fire2_concat"
  type: "Concat"
  bottom: "fire2_expand1x1"
  bottom: "fire2_expand3x3"
  top: "fire2_concat"
}

# ==============================================
layer {
  name: "fire3_squeeze1x1"
  type: "Convolution"
  bottom: "fire2_concat"
  top: "fire3_squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
  }
}
layer {
  name: "fire3_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3_squeeze1x1"
  top: "fire3_squeeze1x1"
}
layer {
  name: "fire3_expand1x1"
  type: "Convolution"
  bottom: "fire3_squeeze1x1"
  top: "fire3_expand1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}
layer {
  name: "fire3_relu_expand1x1"
  type: "ReLU"
  bottom: "fire3_expand1x1"
  top: "fire3_expand1x1"
}
layer {
  name: "fire3_expand3x3"
  type: "Convolution"
  bottom: "fire3_squeeze1x1"
  top: "fire3_expand3x3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire3_relu_expand3x3"
  type: "ReLU"
  bottom: "fire3_expand3x3"
  top: "fire3_expand3x3"
}
layer {
  name: "fire3_concat"
  type: "Concat"
  bottom: "fire3_expand1x1"
  bottom: "fire3_expand3x3"
  top: "fire3_concat"
}

# ==============================================
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3_concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
 }
}

# ==============================================
layer {
  name: "fire4_squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "fire4_squeeze1x1"
  convolution_param {
    num_output: 32
    kernel_size: 1
  }
}
layer {
  name: "fire4_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4_squeeze1x1"
  top: "fire4_squeeze1x1"
}
layer {
  name: "fire4_expand1x1"
  type: "Convolution"
  bottom: "fire4_squeeze1x1"
  top: "fire4_expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
  }
}
layer {
  name: "fire4_relu_expand1x1"
  type: "ReLU"
  bottom: "fire4_expand1x1"
  top: "fire4_expand1x1"
}
layer {
  name: "fire4_expand3x3"
  type: "Convolution"
  bottom: "fire4_squeeze1x1"
  top: "fire4_expand3x3"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire4_relu_expand3x3"
  type: "ReLU"
  bottom: "fire4_expand3x3"
  top: "fire4_expand3x3"
}
layer {
  name: "fire4_concat"
  type: "Concat"
  bottom: "fire4_expand1x1"
  bottom: "fire4_expand3x3"
  top: "fire4_concat"
}

# ==============================================
layer {
  name: "fire5_squeeze1x1"
  type: "Convolution"
  bottom: "fire4_concat"
  top: "fire5_squeeze1x1"
  convolution_param {
    num_output: 32
    kernel_size: 1
  }
}
layer {
  name: "fire5_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5_squeeze1x1"
  top: "fire5_squeeze1x1"
}
layer {
  name: "fire5_expand1x1"
  type: "Convolution"
  bottom: "fire5_squeeze1x1"
  top: "fire5_expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
  }
}
layer {
  name: "fire5_relu_expand1x1"
  type: "ReLU"
  bottom: "fire5_expand1x1"
  top: "fire5_expand1x1"
}
layer {
  name: "fire5_expand3x3"
  type: "Convolution"
  bottom: "fire5_squeeze1x1"
  top: "fire5_expand3x3"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire5_relu_expand3x3"
  type: "ReLU"
  bottom: "fire5_expand3x3"
  top: "fire5_expand3x3"
}
layer {
  name: "fire5_concat"
  type: "Concat"
  bottom: "fire5_expand1x1"
  bottom: "fire5_expand3x3"
  top: "fire5_concat"
}

# ==============================================
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5_concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
 }
}

# ==============================================
layer {
  name: "fire6_squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "fire6_squeeze1x1"
  convolution_param {
    num_output: 48
    kernel_size: 1
  }
}
layer {
  name: "fire6_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6_squeeze1x1"
  top: "fire6_squeeze1x1"
}
layer {
  name: "fire6_expand1x1"
  type: "Convolution"
  bottom: "fire6_squeeze1x1"
  top: "fire6_expand1x1"
  convolution_param {
    num_output: 192
    kernel_size: 1
  }
}
layer {
  name: "fire6_relu_expand1x1"
  type: "ReLU"
  bottom: "fire6_expand1x1"
  top: "fire6_expand1x1"
}
layer {
  name: "fire6_expand3x3"
  type: "Convolution"
  bottom: "fire6_squeeze1x1"
  top: "fire6_expand3x3"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire6_relu_expand3x3"
  type: "ReLU"
  bottom: "fire6_expand3x3"
  top: "fire6_expand3x3"
}
layer {
  name: "fire6_concat"
  type: "Concat"
  bottom: "fire6_expand1x1"
  bottom: "fire6_expand3x3"
  top: "fire6_concat"
}

# ==============================================
layer {
  name: "fire7_squeeze1x1"
  type: "Convolution"
  bottom: "fire6_concat"
  top: "fire7_squeeze1x1"
  convolution_param {
    num_output: 48
    kernel_size: 1
  }
}
layer {
  name: "fire7_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7_squeeze1x1"
  top: "fire7_squeeze1x1"
}
layer {
  name: "fire7_expand1x1"
  type: "Convolution"
  bottom: "fire7_squeeze1x1"
  top: "fire7_expand1x1"
  convolution_param {
    num_output: 192
    kernel_size: 1
  }
}
layer {
  name: "fire7_relu_expand1x1"
  type: "ReLU"
  bottom: "fire7_expand1x1"
  top: "fire7_expand1x1"
}
layer {
  name: "fire7_expand3x3"
  type: "Convolution"
  bottom: "fire7_squeeze1x1"
  top: "fire7_expand3x3"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire7_relu_expand3x3"
  type: "ReLU"
  bottom: "fire7_expand3x3"
  top: "fire7_expand3x3"
}
layer {
  name: "fire7_concat"
  type: "Concat"
  bottom: "fire7_expand1x1"
  bottom: "fire7_expand3x3"
  top: "fire7_concat"
}

# ==============================================
layer {
  name: "fire8_squeeze1x1"
  type: "Convolution"
  bottom: "fire7_concat"
  top: "fire8_squeeze1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}
layer {
  name: "fire8_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8_squeeze1x1"
  top: "fire8_squeeze1x1"
}
layer {
  name: "fire8_expand1x1"
  type: "Convolution"
  bottom: "fire8_squeeze1x1"
  top: "fire8_expand1x1"
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}
layer {
  name: "fire8_relu_expand1x1"
  type: "ReLU"
  bottom: "fire8_expand1x1"
  top: "fire8_expand1x1"
}
layer {
  name: "fire8_expand3x3"
  type: "Convolution"
  bottom: "fire8_squeeze1x1"
  top: "fire8_expand3x3"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire8_relu_expand3x3"
  type: "ReLU"
  bottom: "fire8_expand3x3"
  top: "fire8_expand3x3"
}
layer {
  name: "fire8_concat"
  type: "Concat"
  bottom: "fire8_expand1x1"
  bottom: "fire8_expand3x3"
  top: "fire8_concat"
}

# ==============================================
layer {
  name: "fire9_squeeze1x1"
  type: "Convolution"
  bottom: "fire8_concat"
  top: "fire9_squeeze1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
  }
}
layer {
  name: "fire9_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9_squeeze1x1"
  top: "fire9_squeeze1x1"
}
layer {
  name: "fire9_expand1x1"
  type: "Convolution"
  bottom: "fire9_squeeze1x1"
  top: "fire9_expand1x1"
  convolution_param {
    num_output: 256
    kernel_size: 1
  }
}
layer {
  name: "fire9_relu_expand1x1"
  type: "ReLU"
  bottom: "fire9_expand1x1"
  top: "fire9_expand1x1"
}
layer {
  name: "fire9_expand3x3"
  type: "Convolution"
  bottom: "fire9_squeeze1x1"
  top: "fire9_expand3x3"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire9_relu_expand3x3"
  type: "ReLU"
  bottom: "fire9_expand3x3"
  top: "fire9_expand3x3"
}
layer {
  name: "fire9_concat"
  type: "Concat"
  bottom: "fire9_expand1x1"
  bottom: "fire9_expand3x3"
  top: "fire9_concat"
}

# ==============================================
layer {
  name: "fire10_squeeze1x1"
  type: "Convolution"
  bottom: "fire9_concat"
  top: "fire10_squeeze1x1"
  convolution_param {
    num_output: 96
    kernel_size: 1
  }
}
layer {
  name: "fire10_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire10_squeeze1x1"
  top: "fire10_squeeze1x1"
}
layer {
  name: "fire10_expand1x1"
  type: "Convolution"
  bottom: "fire10_squeeze1x1"
  top: "fire10_expand1x1"
  convolution_param {
    num_output: 384
    kernel_size: 1
  }
}
layer {
  name: "fire10_relu_expand1x1"
  type: "ReLU"
  bottom: "fire10_expand1x1"
  top: "fire10_expand1x1"
}
layer {
  name: "fire10_expand3x3"
  type: "Convolution"
  bottom: "fire10_squeeze1x1"
  top: "fire10_expand3x3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire10_relu_expand3x3"
  type: "ReLU"
  bottom: "fire10_expand3x3"
  top: "fire10_expand3x3"
}
layer {
  name: "fire10_concat"
  type: "Concat"
  bottom: "fire10_expand1x1"
  bottom: "fire10_expand3x3"
  top: "fire10_concat"
}

# ==============================================
layer {
  name: "fire11_squeeze1x1"
  type: "Convolution"
  bottom: "fire10_concat"
  top: "fire11_squeeze1x1"
  convolution_param {
    num_output: 96
    kernel_size: 1
  }
}
layer {
  name: "fire11_relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire11_squeeze1x1"
  top: "fire11_squeeze1x1"
}
layer {
  name: "fire11_expand1x1"
  type: "Convolution"
  bottom: "fire11_squeeze1x1"
  top: "fire11_expand1x1"
  convolution_param {
    num_output: 384
    kernel_size: 1
  }
}
layer {
  name: "fire11_relu_expand1x1"
  type: "ReLU"
  bottom: "fire11_expand1x1"
  top: "fire11_expand1x1"
}
layer {
  name: "fire11_expand3x3"
  type: "Convolution"
  bottom: "fire11_squeeze1x1"
  top: "fire11_expand3x3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "fire11_relu_expand3x3"
  type: "ReLU"
  bottom: "fire11_expand3x3"
  top: "fire11_expand3x3"
}
layer {
  name: "fire11_concat"
  type: "Concat"
  bottom: "fire11_expand1x1"
  bottom: "fire11_expand3x3"
  top: "fire11_concat"
}

# ==============================================
layer {
  name: "drop11"
  type: "Dropout"
  bottom: "fire11_concat"
  top: "drop11"
  dropout_param {
    dropout_ratio: 1.0
  }
}

# ==============================================
# Prediction Layer
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "drop11"
  top: "conv12"
  convolution_param {
    num_output: 117
    pad: 1
    kernel_size: 3
    stride: 1
  }
}

# ==============================================
#========== slicing layer
# input layer is assumed to be [n_anchors*n_classes + n_anchors + n_anchors*4] -> (9*8 + 9 + 36) -> 117

layer {
  name: "slicer_conv12"
  type: "Slice"
  bottom: "conv12"
  top: "slice_pred_class_probs"
  top: "slice_pred_confidence_probs"
  top: "slice_pred_box_delta"

  slice_param {
    axis: 1
    # first slice is from 0:72 no need to specify here (classes probs)
    # [n_anchors*n_classes] = (9*8 = 72)

    # second slice is from 72:81 specify only the start (anchors confidences)
    # [n_anchors] (9)
    slice_point: 72

    # third slice is from 81:117 only specify the start (Bboxes)
    # [n_anchors*4] (9*4 = 36)
    slice_point: 81 
  }
}

# ==============================================
#========== Class probabilities
layer{
    name: "permute_slice_pred_class_probs"
    type: "Permute"
    bottom: "slice_pred_class_probs"
    top: "permute_slice_pred_class_probs"
    permute_param{
        order: 0
        order: 2
        order: 3
        order: 1
    }
}

# output from here is [1, 9x30x40, 8]
layer {
    name: "reshape_slice_pred_class_probs"
    type: "Reshape"
    bottom: "permute_slice_pred_class_probs"
    top: "reshape_slice_pred_class_probs"
    reshape_param {
      shape {
        dim: -1
        dim: 8
      }
    }
}

layer {
  name: "soft_pred_class_probs"
  type: "Softmax"
  bottom: "reshape_slice_pred_class_probs"
  top: "soft_pred_class_probs"
}

layer {
    name: "pred_class_probs"
    type: "Reshape"
    bottom: "soft_pred_class_probs"
    top: "pred_class_probs"
    reshape_param {
      shape {
        dim: 1 # batch size
        dim: -1
        dim: 8 # how many classes
      }
    }
}

# ==============================================
#============= object confidences
layer{
    name: "permute_slice_pred_conf"
    type: "Permute"
    bottom: "slice_pred_confidence_probs"
    top: "permute_slice_pred_conf"
    permute_param{
        order: 0
        order: 2
        order: 3
        order: 1
    }
}

# output from here is [1, 30x40x9]
layer {
    name: "reshape_slice_pred_conf"
    type: "Reshape"
    bottom: "permute_slice_pred_conf"
    top: "reshape_slice_pred_conf"
    reshape_param {
      shape {
        dim: 1 # batch size
        dim: -1 
      }
    }
}

layer {
  name: "pred_conf"
  type: "Sigmoid"
  bottom: "reshape_slice_pred_conf"
  top: "pred_conf"
}

# ==============================================
#=============== Bbox delta values
layer{
    name: "permute_slice_pred_box_delta"
    type: "Permute"
    bottom: "slice_pred_box_delta"
    top: "permute_slice_pred_box_delta"
    permute_param{
        order: 0
        order: 2
        order: 3
        order: 1
    }
}

# output from here is [1, 9x30x40, 4]
layer {
    name: "reshape_slice_pred_box_delta"
    type: "Reshape"
    bottom: "permute_slice_pred_box_delta"
    top: "reshape_slice_pred_box_delta"
    reshape_param {
      shape {
        dim: 1 # batch size
        dim: -1
        dim: 4 # how many points in the Bbox
      }
    }
}


































